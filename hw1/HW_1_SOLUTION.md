### Этап №1: Думаем и проектируем
Начнем с железа: смотрим использование ресурсов — возможен дефицит CPU или утечки памяти. Метрики инфраструктуры:
\ CPU usage 
\ Memory usage 
\ Load Average
\ Network traffic
\ Disk IO 
Метрики бека:
\\ общее число запросов
\ число ошибок (5xx)
\ RPS
\ latency (p95, p99) 
Потенциально узкий участок БД:
\ количество активных коннектов
\ QPS (queries/sec)
\ длительность запросов к БД
\ cache hit ratio
### Этап №2: Стреляем и анализируем
Поднимаем приложение:
![[Pasted image 20251228133711.png]]
Проверяем статус контейнеров:
![[Pasted image 20251228133738.png]]
Меняем uid в конфигах дашборда на корректные, проверяем, что дашборды графаны тянут данные из прометея:
![[Pasted image 20251228133910.png]]
![[Pasted image 20251228133923.png]]![[Pasted image 20251228133934.png]]
Нагрузка системы в простое не скачет, можем приступать к тестированию.

Создаем двух юзеров для тестирования:
![[Pasted image 20251228133957.png]]

#### Сценарий шторм (резкий рост до 1000 юзеров за 8 секунд):
![[Pasted image 20251228134133.png]]
Запускаем, смотрим на метрики:
![[Pasted image 20251228134200.png]]

**Анализ железа**
![[Pasted image 20251228134514.png]]
По железу и памяти не упираемся в потолок, проходим с запасом. Поблемы не на уровне железа или оси.

**Анализ k6**
![[Pasted image 20251228134506.png]]
Метрики:
Всего HTTP-запросов: 78 029
HTTP request failures: 9 900 (~12.7%)
Peak RPS: ~1.98k req/s
p99 latency: ~1.02 s
VUs: рост до 1000, затем резкий скачок p99

Выводы:
1. Система вошла в насыщение (p99 резко скачет вверх)
2. 13% ошибок выглядит как перегруз системы, а не как случайные ошибки
3. Предполагаем ограниченный downstream ресурс

**Анализ БД**
![[Pasted image 20251228134519.png]]
Метрики:
QPS: ~321
Fetched / Returned rows: пик до ~150–160k строк
Inserted/Updated/Deleted: ≈ 0
Active connections: Резкий пик до ~55–60 активных соединений, до шторма почти ноль
Cache hit ratio: Почти 100%, просадка в начале, затем восстановление

Выводы:
Postgres не загружен по CPU или I/O, но количество соединений резко растёт, а QPS в БД (321) в 6 раз меньше, чем входной HTTP RPS (~2k). Вероятно, проблемы с приложением связаны с БД. 

#### Сценарий волна (плавный рост до 500 юзеров за 1.5 минуты):
![[Pasted image 20251228135333.png]]
![[Pasted image 20251228135416.png]]

![[Pasted image 20251228135747.png]]
![[Pasted image 20251228135756.png]]
Наблюдения по железу:
Система начинает деградировать по latency и количеству ошибок при росте нагрузки.
Ограничения наблюдаются не в памяти и не в сети.
Load > 100% при CPU < 50% указывает на наличие блокировок.
Нет резкого роста памяти, что говорит об отсутствии утечек.
Использование диска и сети остается стабильным.
После 300-350 VUs и 600 rps наблюдается резкий рост latency и появляются ошибки. Скорее всего, проблема кроется в запросах к БД.

Наблюдения по БД:
Увеличение DB connections приводит к росту блокировок бэкэнда.
QPS во время тестирования примерно 1300. Это очень много для Postgres. 

#### Выводы
Основным бутылочным горлышком является БД: резкий рост количества возвращаемых строк и активных соединений приводит к блокировке горутин в бэкэнд-приложении. Это вызывает увеличение **latency** (p99 > **1 с**), рост числа ошибок (~**11%**) и высокий load average при относительно низкой загрузке CPU. Память, сеть и диск не являются ограничивающими факторами.

### Этап №3: Решения

Бекенд:
- Реализовать ограничение на количество одновременно открытых подключений к базе данных
- Настроить таймауты для запросов к БД

Архитектура:
- Внедрить read-only реплику базы данных для распределения нагрузки на чтение
- Использовать пул соединений, такой как pgBouncer, чтобы эффективно управлять соединениями и снизить количество одновременно открытых
